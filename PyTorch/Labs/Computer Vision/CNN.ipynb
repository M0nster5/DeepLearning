{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=center><font size = 5>Convolutional Neral Network Simple example </font></h1> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f4870edf5d0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_channels(W):\n",
    "    #number of output channels \n",
    "    n_out=W.shape[0]\n",
    "    #number of input channels \n",
    "    n_in=W.shape[1]\n",
    "    w_min=W.min().item()\n",
    "    w_max=W.max().item()\n",
    "    fig, axes = plt.subplots(n_out,n_in)\n",
    "    fig.subplots_adjust(hspace = 0.1)\n",
    "    out_index=0\n",
    "    in_index=0\n",
    "    #plot outputs as rows inputs as columns \n",
    "    for ax in axes.flat:\n",
    "    \n",
    "        if in_index>n_in-1:\n",
    "            out_index=out_index+1\n",
    "            in_index=0\n",
    "              \n",
    "        ax.imshow(W[out_index,in_index,:,:], vmin=w_min, vmax=w_max, cmap='seismic')\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_xticklabels([])\n",
    "        in_index=in_index+1\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_data(dataset,sample):\n",
    "\n",
    "    plt.imshow(dataset.x[sample,0,:,:].numpy(),cmap='gray')\n",
    "    plt.title('y='+str(dataset.y[sample].item()))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "class Data(Dataset):\n",
    "    def __init__(self,N_images=100,offset=0,p=0.9, train=False):\n",
    "        \"\"\"\n",
    "        p:portability that pixel is wight  \n",
    "        N_images:number of images \n",
    "        offset:set a random vertical and horizontal offset images by a sample should be less than 3 \n",
    "        \"\"\"\n",
    "        if train==True:\n",
    "            np.random.seed(1)  \n",
    "        \n",
    "        #make images multiple of 3 \n",
    "        N_images=2*(N_images//2)\n",
    "        images=np.zeros((N_images,1,11,11))\n",
    "        start1=3\n",
    "        start2=1\n",
    "        self.y=torch.zeros(N_images).type(torch.long)\n",
    "\n",
    "        for n in range(N_images):\n",
    "            if offset>0:\n",
    "        \n",
    "                low=int(np.random.randint(low=start1, high=start1+offset, size=1))\n",
    "                high=int(np.random.randint(low=start2, high=start2+offset, size=1))\n",
    "            else:\n",
    "                low=4\n",
    "                high=1\n",
    "        \n",
    "            if n<=N_images//2:\n",
    "                self.y[n]=0\n",
    "                images[n,0,high:high+9,low:low+3]= np.random.binomial(1, p, (9,3))\n",
    "            elif  n>N_images//2:\n",
    "                self.y[n]=1\n",
    "                images[n,0,low:low+3,high:high+9] = np.random.binomial(1, p, (3,9))\n",
    "           \n",
    "        \n",
    "        \n",
    "        self.x=torch.from_numpy(images).type(torch.FloatTensor)\n",
    "        self.len=self.x.shape[0]\n",
    "        del(images)\n",
    "        np.random.seed(0)\n",
    "    def __getitem__(self,index):      \n",
    "        return self.x[index],self.y[index]\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_activations(A,number_rows= 1,name=\"\"):\n",
    "    A=A[0,:,:,:].detach().numpy()\n",
    "    n_activations=A.shape[0]\n",
    "    \n",
    "    \n",
    "    print(n_activations)\n",
    "    A_min=A.min().item()\n",
    "    A_max=A.max().item()\n",
    "\n",
    "    if n_activations==1:\n",
    "\n",
    "        # Plot the image.\n",
    "        plt.imshow(A[0,:], vmin=A_min, vmax=A_max, cmap='seismic')\n",
    "\n",
    "    else:\n",
    "        fig, axes = plt.subplots(number_rows, n_activations//number_rows)\n",
    "        fig.subplots_adjust(hspace = 0.4)\n",
    "        for i,ax in enumerate(axes.flat):\n",
    "            if i< n_activations:\n",
    "                # Set the label for the sub-plot.\n",
    "                ax.set_xlabel( \"activation:{0}\".format(i+1))\n",
    "\n",
    "                # Plot the image.\n",
    "                ax.imshow(A[i,:], vmin=A_min, vmax=A_max, cmap='seismic')\n",
    "                ax.set_xticks([])\n",
    "                ax.set_yticks([])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def conv_output_shape(h_w, kernel_size=1, stride=1, pad=0, dilation=1):\n",
    "    #by Duane Nielsen\n",
    "    from math import floor\n",
    "    if type(kernel_size) is not tuple:\n",
    "        kernel_size = (kernel_size, kernel_size)\n",
    "    h = floor( ((h_w[0] + (2 * pad) - ( dilation * (kernel_size[0] - 1) ) - 1 )/ stride) + 1)\n",
    "    w = floor( ((h_w[1] + (2 * pad) - ( dilation * (kernel_size[1] - 1) ) - 1 )/ stride) + 1)\n",
    "    return h, w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ref1\"></a>\n",
    "<h2 align=center>Prepare Data </h2> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_images=10000\n",
    "train_dataset=Data(N_images=N_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Data at 0x7f4822557f60>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_dataset=Data(N_images=1000,train=False)\n",
    "validation_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Visualization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_data(train_dataset,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_data(train_dataset,N_images//2+2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ref3\"></a>\n",
    "### Build a Convolutional Neral Network Class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 10)\n",
      "(9, 9)\n",
      "(8, 8)\n",
      "(7, 7)\n"
     ]
    }
   ],
   "source": [
    "out=conv_output_shape((11,11), kernel_size=2, stride=1, pad=0, dilation=1)\n",
    "print(out)\n",
    "out1=conv_output_shape(out, kernel_size=2, stride=1, pad=0, dilation=1)\n",
    "print(out1)\n",
    "out2=conv_output_shape(out1, kernel_size=2, stride=1, pad=0, dilation=1)\n",
    "print(out2)\n",
    "\n",
    "out3=conv_output_shape(out2, kernel_size=2, stride=1, pad=0, dilation=1)\n",
    "print(out3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self,out_1=2,out_2=1):\n",
    "        \n",
    "        super(CNN,self).__init__()\n",
    "        #first Convolutional layers \n",
    "        self.cnn1=nn.Conv2d(in_channels=1,out_channels=out_1,kernel_size=2,padding=0)\n",
    "        #activation function \n",
    "        self.relu1=nn.ReLU()\n",
    "        #max pooling \n",
    "        self.maxpool1=nn.MaxPool2d(kernel_size=2 ,stride=1)\n",
    "        #second Convolutional layers\n",
    "        self.cnn2=nn.Conv2d(in_channels=out_1,out_channels=out_2,kernel_size=2,stride=1,padding=0)\n",
    "        #activation function \n",
    "        self.relu2=nn.ReLU()\n",
    "        #max pooling \n",
    "        self.maxpool2=nn.MaxPool2d(kernel_size=2 ,stride=1)\n",
    "        #fully connected layer \n",
    "        self.fc1=nn.Linear(out_2*7*7,2)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        #first Convolutional layers\n",
    "        out=self.cnn1(x)\n",
    "        #activation function \n",
    "        out=self.relu1(out)\n",
    "        #max pooling \n",
    "        out=self.maxpool1(out)\n",
    "        #first Convolutional layers\n",
    "        out=self.cnn2(out)\n",
    "        #activation function\n",
    "        out=self.relu2(out)\n",
    "        #max pooling\n",
    "        out=self.maxpool2(out)\n",
    "        #flatten output \n",
    "        out=out.view(out.size(0),-1)\n",
    "        #fully connected layer\n",
    "        out=self.fc1(out)\n",
    "        return out\n",
    "    \n",
    "    def activations(self,x):\n",
    "        #outputs activation this is not necessary just for fun \n",
    "        z1=self.cnn1(x)\n",
    "        a1=self.relu1(z1)\n",
    "        out=self.maxpool1(a1)\n",
    "        \n",
    "        z2=self.cnn2(out)\n",
    "        a2=self.relu2(z2)\n",
    "        out=self.maxpool2(a2)\n",
    "        out=out.view(out.size(0),-1)\n",
    "        return z1,a1,z2,a2,out        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ref3\"></a>\n",
    "<h2> Define the Convolutional Neral Network Classifier , Criterion function, Optimizer and Train the  Model  </h2> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=CNN(2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (cnn1): Conv2d(1, 2, kernel_size=(2, 2), stride=(1, 1))\n",
       "  (relu1): ReLU()\n",
       "  (maxpool1): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "  (cnn2): Conv2d(2, 1, kernel_size=(2, 2), stride=(1, 1))\n",
       "  (relu2): ReLU()\n",
       "  (maxpool2): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): Linear(in_features=49, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHwAAADuCAYAAAD7sGP2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAA8FJREFUeJzt3T9uE0EUgPH3+CMFgkSKhILGZdxnW8QxuIEPwVF8Cy5BZfqkQZSRcOEiBd2joAkSYljFs7vh+371SHmjT55VIk82qyrE8WTuATQtg8MYHMbgMAaHMTiMwWEMDmNwmGetBZm5iYhNRMTpycnVerXqPpTG+3Z7G/vDIVvrcsyfVof1unbb7YMGUx/DZhO76+tmcI90GIPDGBzG4DAGhzE4jMFhDA5jcBiDwxgcxuAwBocxOIzBYQwOY3AYg8MYHMbgMAaHMTiMwWEMDtMMnpmbzNxl5u774TDFTOqoGbyqtlU1VNVwcXY2xUzqyCMdxuAwBocxOIzBYQwOY3AYg8MYHMbgMAaHMTiMwWEMDmNwGIPDGBzG4DAGhzE4jMFhDA5jcJjmKzDu+3JzF/n+c69ZZvBh7gGO6Pk/rRr1zpMILyI8dqNunkScTjGTOvIZDmNwGIPDGBzG4DAGhzE4jMFhDA5jcBiDwxgcxuAwBocxOIzBYQwOY3AYg8MYHMbgMAaHMTiMwWG8eQLjzRMYj3QYg8MYHMbgMAaHMTiMwWEMDmNwGIPDGBzG4DAGhzE4jMFhDA5jcBiDwxgcxuAwBocxOIzBYbKq/r7gt4sIcRkRN72HiojziNhP8HOmMNVeVlV10VrUDD6HzNz9uvjw+C1tLx7pMAaHWWrw7dwDHNGi9rLIZ7j6WeonXJ0YHMbgMAaHMTjMyP/x8vQq4lXnkaY06m3aC3cXVT+ytWrUr2WZZxXx7kFjLcubuQc4ok9RtW8G90iHMTiMwWEMDmNwGIPDGBzG4DAGhzE4jMFhDA5jcBiDwxgcxuAwBocxOIzBYQwOY3AYg8MYHMbgMCNvnrzoPI568+bJf8ObJ/oDg8MYHMbgMAaHMTiMwWEMDmNwGIPDGBzG4DAGhzE4jMFhDA5jcBiDwxgcxuAwBocxOIzBYUZ9L/1tZm3ayx6N9dwDHNHHiPha1fxe+qibJ68fPpdm1jzSq2pbVUNVDS+nmEhd+QyHMTiMwWEMDmNwGIPDGBzG4DAGhzE4jMFhDA5jcBiDwxgcxuAwBocxOIzBYQwOY3AYg8MYHMaLCDBeRIDxSIcxOIzBYQwOY3AYg8MYHMbgMAaHMTiMwWEMDmNwGIPDGBzG4DAGhzE4jMFhDA5jcBiDwxgcxuAwzXee3L95EhGXEXHTe6iIOI+I/QQ/ZwpT7WVVVRetRaNecjOVzNxV1TD3HMewtL14pMMYHGapwbdzD3BEi9rLIp/h6mepn3B1YnAYg8MYHMbgMD8BE9OkhYHGPEYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plot_channels(model.state_dict()['cnn1.weight'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWQAAACtCAYAAACHiwXoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAABDdJREFUeJzt2zFqXFcYhuH/BBVBwQiM0oS4sQlOETfxLCNNNnF3kH2kDCq9A28hbqezyrQBg42wG4Pd/CkswwRCZkZi7v1Gfp5OcOF8iMOLOKDR3QXA8r5aegAAnwgyQAhBBgghyAAhBBkghCADhBBkgBCCDBBCkAFCnGz7YIwxVdX06aevn1Z9f+BJd8nrpQccmffV/XHMcdLmvf7m9PTpjw8fznHsnfDh8nLpCUfl76q66t7pXo99/nV6jB+66veb7voC/bH0gCPzorrfzhLkTasnT3r9/Pncxx6tvx49WnrCUfm1ql7uGGRPFgAhBBkghCADhBBkgBCCDBBCkAFCCDJACEEGCCHIACEEGSCEIAOEEGSAEIIMEEKQAUIIMkAIQQYIIcgAIQQZIIQgA4QQZIAQggwQQpABQggyQAhBBgghyAAhBBkghCADhBBkgBCCDBBCkAFCCDJACEEGCCHIACEEGSCEIAOEEGSAEFuDPMaYxhjrMca66t0cm+DgNu/166urpedAVe0Q5O6+6O5Vd6+qzubYBAe3ea+/vX9/6TlQVZ4sAGIIMkAIQQYIIcgAIQQZIIQgA4QQZIAQggwQQpABQggyQAhBBgghyAAhBBkghCADhBBkgBCCDBBCkAFCCDJACEEGCCHIACEEGSCEIAOEEGSAEIIMEEKQAUIIMkAIQQYIIcgAIQQZIIQgA4QQZIAQggwQQpABQggyQAhBBggxunvnj78bo6cDjrlr3v+2+++WqmfPVvXq1XrMfe7PY/Sfcx96xO7VL0tPODIvqvvtTvf6ZNsHY4ypqqaqqrNbzoIUm/f6wcJb4LOtTxbdfdHdq+5enc6xCGawea/Plx4D17whA4QQZIAQggwQQpABQggyQAhBBgghyAAhBBkghCADhBBkgBCCDBBCkAFCCDJACEEGCCHIACEEGSCEIAOEEGSAEIIMEEKQAUIIMkAIQQYIIcgAIQQZIIQgA4QQZIAQggwQQpABQggyQAhBBgghyAAhBBkghCADhBBkgBCCDBBCkAFCnGz7YIwxVdVUVXV28Dkwj817/WDhLfDZ1r+Qu/uiu1fdvTqdYxHMYPNeny89Bq55sgAIIcgAIQQZIIQgA4QQZIAQggwQQpABQggyQAhBBgghyAAhBBkghCADhBBkgBCCDBBCkAFCCDJACEEGCCHIACEEGSCEIAOEEGSAEIIMEEKQAUIIMkAIQQYIIcgAIQQZIIQgA4QQZIAQggwQQpABQggyQAhBBgghyAAhBBkghCADhBjd/f8fjDFV1XT9409VdXnoUTdwXlVvlh7xH+zaz+PuvjfHQe71rdi1n53v9dYg/+vjMdbdvbrxrAOxaz92ZZy7jV37uQu7PFkAhBBkgBD7BvniICtuz6792JVx7jZ27efod+31hgzA4XiyAAghyAAhBBkghCADhBBkgBD/ABM6wwHlkB91AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_channels(model.state_dict()['cnn2.weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion=nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate=0.001\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_loader=torch.utils.data.DataLoader(dataset=train_dataset,batch_size=10)\n",
    "validation_loader=torch.utils.data.DataLoader(dataset=validation_dataset,batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs=10\n",
    "loss_list=[]\n",
    "accuracy_list=[]\n",
    "N_test=len(validation_dataset)\n",
    "#n_epochs\n",
    "for epoch in range(n_epochs):\n",
    "        \n",
    "    for x, y in train_loader:\n",
    "      \n",
    "\n",
    "        #clear gradient \n",
    "        optimizer.zero_grad()\n",
    "        #make a prediction \n",
    "        z=model(x)\n",
    "        # calculate loss \n",
    "        loss=criterion(z,y)\n",
    "        # calculate gradients of parameters \n",
    "        loss.backward()\n",
    "        # update parameters \n",
    "        optimizer.step()\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "    correct=0\n",
    "    #perform a prediction on the validation  data  \n",
    "    for x_test, y_test in validation_loader:\n",
    "\n",
    "        z=model(x_test)\n",
    "        _,yhat=torch.max(z.data,1)\n",
    "\n",
    "        correct+=(yhat==y_test).sum().item()\n",
    "        \n",
    "\n",
    "    accuracy=correct/N_test\n",
    "\n",
    "    accuracy_list.append(accuracy)\n",
    "    \n",
    "    loss_list.append(loss.data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id=\"ref3\"></a>\n",
    "<h2 align=center>Analyse Results</h2> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "color = 'tab:red'\n",
    "ax1.plot(loss_list,color=color)\n",
    "ax1.set_xlabel('epoch',color=color)\n",
    "ax1.set_ylabel('total loss',color=color)\n",
    "ax1.tick_params(axis='y', color=color)\n",
    "    \n",
    "ax2 = ax1.twinx()  \n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('accuracy', color=color)  \n",
    "ax2.plot( accuracy_list, color=color)\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.state_dict()['cnn1.weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_channels(model.state_dict()['cnn1.weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.state_dict()['cnn1.weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_channels(model.state_dict()['cnn2.weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_data(train_dataset,N_images//2+2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out=model.activations(train_dataset[N_images//2+2][0].view(1,1,11,11))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_activations(out[0],number_rows=1,name=\"first feature map\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_activations(out[2],number_rows=1,name=\"first feature map\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_activations(out[3],number_rows=1,name=\"first feature map\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out1=out[4][0].detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out0=model.activations(train_dataset[100][0].view(1,1,11,11))[4][0].detach().numpy()\n",
    "out0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(2, 1, 1)\n",
    "plt.plot( out1, 'b')\n",
    "plt.title('Flatted Activation Values  ')\n",
    "plt.ylabel('Activation')\n",
    "plt.xlabel('index')\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(out0, 'r')\n",
    "plt.xlabel('index')\n",
    "plt.ylabel('Activation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
